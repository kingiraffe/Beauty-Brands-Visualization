{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changjiang Securities\n",
    "## 2019.04.08 - 2019.06.17\n",
    "As an intern of Industry Research Department in Changjiang Securities, I am mainly responsible for data collection and supporting, our team's focus was on retails including shopping malls, beauty brands, baby cares, etc.To quickly attain the number of stores for large chain brands, a scrapper was written to automatically calculate from the Baidu Map, greatly improving our team's efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count:  1282\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def Crawl(url):\n",
    "    headers = {\"Accept\": \"*/*\",\n",
    "          'Accept-Encoding': 'gzip, deflate, br',\n",
    "          'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',\n",
    "          'Connection': 'keep-alive',\n",
    "          'Host': 'map.baidu.com',\n",
    "          'Referer: https': '//map.baidu.com/search/%E4%BC%98%E8%A1%A3%E5%BA%93/@13432005.56,3644785.89,13z?querytype=s&c=224&wd=%E4%BC%98%E8%A1%A3%E5%BA%93&da_src=shareurl&on_gel=1&l=13&gr=1&b=(13401285.56,3629281.89;13462725.56,3660289.89)&pn=0&device_ratio=2',\n",
    "          'Sec-Fetch-Dest': 'empty',\n",
    "          'Sec-Fetch-Mode': 'cors',\n",
    "          'Sec-Fetch-Site': 'same-origin',\n",
    "          'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36'\n",
    "          }\n",
    "    r = requests.get(url, headers)\n",
    "    return r.json()\n",
    "\n",
    "def getData(url):\n",
    "    currentInfo = []\n",
    "    moreInfo = []\n",
    "    data = Crawl(url)\n",
    "    current_province = data['current_city']['up_province_name']\n",
    "    current_city_info = data['content']\n",
    "    other_city_info = data['more_city']\n",
    "    for city in current_city_info:\n",
    "        l = []\n",
    "        city_name = city['view_name']\n",
    "        city_num = city['num']\n",
    "        l.append(city_name)\n",
    "        l.append(city_name)\n",
    "        l.append(city_num)\n",
    "        currentInfo.append(l)\n",
    "    for other_cities in other_city_info:\n",
    "        province = other_cities['province']\n",
    "        cities = other_cities['city']\n",
    "        for city in cities:\n",
    "            l = []\n",
    "            other_city_name = city['name']\n",
    "            other_city_num = city['num']\n",
    "            l.append(province)\n",
    "            l.append(other_city_name)\n",
    "            l.append(other_city_num)\n",
    "            moreInfo.append(l)\n",
    "    return currentInfo + moreInfo\n",
    "\n",
    "def sumData(url):\n",
    "    Info = getData(url)\n",
    "    Info_df = pd.DataFrame(Info)\n",
    "    Info_sum = Info_df[2].sum()\n",
    "    return Info_sum\n",
    "\n",
    "def getUrl(target_name):\n",
    "    target_store = urllib.parse.quote(target_name, encoding = 'utf-8', errors = 'replace')\n",
    "    target_url = 'https://map.baidu.com/?newmap=1&reqflag=pcmap&biz=1&from=webmap&da_par=after_baidu&pcevaname=pc4.1&qt=s&c=1&wd=' + target_store + '&da_src=pcmappg.map&on_gel=1&l=5&gr=1&b=(10791381.798845354,3109633.7229259782;14055506.524756543,6852939.549588665)&&pn=0&auth=SV3aQL689PMv9fKZaae0D85TyFLzK5xeuxHTxNNBBNVtDpnSCE%40%40B1GgvPUDZYOYIZuVt1cv3uVtGccZcuVtPWv3Guxtdw8E62qvMuTa4AZzUvhgMZSguxzBEHLNRTVtcEWe1GD8zv7u%40ZPuxtfvAughxehwzJVzPPDD4BvgjLLwWvrZZWuB&device_ratio=2&tn=B_NORMAL_MAP&nn=0&u_loc=13437544,3654207&ie=utf-8&t=1591773370952'\n",
    "    return target_url\n",
    "\n",
    "def startCrawl(target_name):\n",
    "    target_url = getUrl(target_name)\n",
    "    return sumData(target_url)\n",
    "\n",
    "def hotCount(target_name):\n",
    "    hot_list = []\n",
    "    target_url = getUrl(target_name)\n",
    "    data = Crawl(target_url)\n",
    "    hot_data = data['hot_city']\n",
    "    for hot_city in hot_data:\n",
    "        hot_count = int(hot_city[hot_city.find('|')+1: ])\n",
    "        hot_list.append(hot_count)\n",
    "    hot_sum = sum(hot_list)\n",
    "    return hot_sum, hot_data\n",
    "\n",
    "## change the target_name, returns the stores count\n",
    "target_name = '优衣库'\n",
    "try:\n",
    "    print('Total count: ', startCrawl(target_name))\n",
    "except:\n",
    "    print('Hot_city count: ', hotCount(target_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide a convenient view of the brands specially focused on, a scrapper was written to gather the data from JD.com including each brand's sales volume, prices distribution and shop types, with Tableau utilized for better visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from openpyxl import load_workbook\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "options = webdriver.ChromeOptions() # declare a Chrome Driver\n",
    "options.add_experimental_option('prefs', {'profile.managed_default_content_settings.images': 2}) # set not to display pictures\n",
    "browser = webdriver.Chrome(options= options)\n",
    "\n",
    "url = 'https://www.jd.com/'\n",
    "\n",
    "def startScraper(keywords):\n",
    "    data_list = [] # declare a list to store dicts\n",
    "    browser.get(url) # request url\n",
    "    browser.find_element_by_id('key').send_keys(keywords) # enter keywords\n",
    "    browser.find_element_by_id('key').send_keys(Keys.ENTER) # start search\n",
    "    WebDriverWait(browser, 1000).until(\n",
    "        EC.presence_of_all_elements_located(\n",
    "            (By.CLASS_NAME, 'pn-next')\n",
    "        )\n",
    "    ) # wait till all the elements all loaded\n",
    "    all_page = eval(browser.find_element_by_css_selector('span.p-skip em b').text) # get #num of pages\n",
    "    count = 0 # set a counter\n",
    "    # loop before the last page\n",
    "    while True:\n",
    "        try:\n",
    "            count += 1\n",
    "            # wait till all the information loaded\n",
    "            WebDriverWait(browser, 1000).until(\n",
    "                EC.presence_of_all_elements_located(\n",
    "                    (By.CLASS_NAME, 'gl-item')\n",
    "                )\n",
    "            )\n",
    "            browser.execute_script('document.documentElement.scrollTop=10000') # draw the bar to the bottom to load all the information\n",
    "            time.sleep(random.randint(1, 3)) # randomly stop for seconds\n",
    "            browser.execute_script('document.documentElement.scrollTop=0') # back to the top\n",
    "            \n",
    "            # extract information from label li\n",
    "            lis = browser.find_elements_by_class_name('gl-item')\n",
    "            for li in lis:\n",
    "                # name\n",
    "                name = li.find_element_by_xpath('.//div[@class=\"p-name p-name-type-2\"]//em').text\n",
    "                # price\n",
    "                price = li.find_element_by_xpath('.//div[@class=\"p-price\"]//i').text\n",
    "                # #num of comments\n",
    "                comment = li.find_elements_by_xpath('.//div[@class=\"p-commit\"]//a')\n",
    "                if comment:\n",
    "                    comment = comment[0].text\n",
    "                else:\n",
    "                    comment = None\n",
    "                # shop name\n",
    "                shop_name = li.find_elements_by_class_name('J_im_icon')\n",
    "                if shop_name:\n",
    "                    shop_name = shop_name[0].text\n",
    "                else:\n",
    "                    shop_name = None\n",
    "                # shop type\n",
    "                shop_type = li.find_elements_by_class_name('goods-icons')\n",
    "                if shop_type:\n",
    "                    shop_type = shop_type[0].text\n",
    "                else:\n",
    "                    shop_type = None\n",
    "                    \n",
    "                # declare a dict to store data\n",
    "                data_dict = {}\n",
    "                data_dict['name'] = name\n",
    "                data_dict['price'] = price\n",
    "                data_dict['comment'] = comment\n",
    "                data_dict['shop_name'] = shop_name\n",
    "                data_dict['shop_type'] = shop_type\n",
    "                \n",
    "                data_list.append(data_dict)\n",
    "                #print(data_dict)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        if count == all_page:\n",
    "            break\n",
    "        # find element for next page and click\n",
    "        fp_next = browser.find_element_by_css_selector('a.fp-next')\n",
    "        browser.execute_script('document.documentElement.scrollTop = 10000')\n",
    "        fp_next.click()\n",
    "    return data_list\n",
    "\n",
    "# prune the 'comments' to 'sales'\n",
    "def prune_sales(comments):\n",
    "    if comments == None:\n",
    "        return int(0)\n",
    "    elif comments == '':\n",
    "        return int(0)\n",
    "    elif comments[-1] != '+':\n",
    "        return int(comments)\n",
    "    elif comments[-2] == '万':\n",
    "        return int(float(comments[:-2]) * 1e4)\n",
    "    else:\n",
    "        return int(comments[:-1])\n",
    "    \n",
    "# save data in excel\n",
    "def main(keywords):\n",
    "    result = startScraper(keywords)\n",
    "    result_df = pd.DataFrame(result)\n",
    "    result_df['sales'] = result_df['comment'].apply(prune_sales)\n",
    "    with pd.ExcelWriter('sku_sells.xlsx', engine = 'openpyxl') as writer:\n",
    "        writer.book = load_workbook('sku_sells.xlsx')\n",
    "        result_df.to_excel(writer, sheet_name = keywords)\n",
    "        \n",
    "keywords = \"雅诗兰黛\" # enter keywords\n",
    "main(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Beauty Brands Dashboard-1.png'>\n",
    "<img src='Beauty Brands Dashboard-2.png'>\n",
    "<img src='Beauty Brands Dashboard-3.png'>\n",
    "<img src='Beauty Brands Dashboard-4.png'>\n",
    "<img src='Beauty Brands Dashboard-5.png'>\n",
    "<img src='Beauty Brands Dashboard-6.png'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
